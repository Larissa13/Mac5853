{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Importing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-02T15:20:35.331727Z",
     "start_time": "2018-11-02T15:20:19.970179Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /home/luke/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package rslp to /home/luke/nltk_data...\n",
      "[nltk_data]   Package rslp is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to /home/luke/nltk_data...\n",
      "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
      "Requirement already satisfied: pt_core_news_sm==2.0.0 from https://github.com/explosion/spacy-models/releases/download/pt_core_news_sm-2.0.0/pt_core_news_sm-2.0.0.tar.gz#egg=pt_core_news_sm==2.0.0 in /home/luke/anaconda3/envs/text-rec/lib/python3.6/site-packages (2.0.0)\n",
      "\u001b[33mYou are using pip version 10.0.1, however version 18.1 is available.\n",
      "You should consider upgrading via the 'pip install --upgrade pip' command.\u001b[0m\n",
      "\n",
      "\u001b[93m    Linking successful\u001b[0m\n",
      "    /home/luke/anaconda3/envs/text-rec/lib/python3.6/site-packages/pt_core_news_sm\n",
      "    -->\n",
      "    /home/luke/anaconda3/envs/text-rec/lib/python3.6/site-packages/spacy/data/pt\n",
      "\n",
      "    You can now load the model via spacy.load('pt')\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "nltk.download('rslp')\n",
    "nltk.download('punkt')\n",
    "!python -m spacy download pt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-02T15:20:35.343860Z",
     "start_time": "2018-11-02T15:20:35.338457Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import RSLPStemmer \n",
    "from nltk import tokenize\n",
    "import spacy\n",
    "import string, re"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pre-processing the text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-02T15:20:35.445651Z",
     "start_time": "2018-11-02T15:20:35.347139Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def remove_stopwords(text):\n",
    "    new_text = []\n",
    "    stop_words = set(stopwords.words('portuguese'))\n",
    "    for word in text:\n",
    "        if word not in stop_words:\n",
    "            new_text += [word]\n",
    "    return new_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-02T15:20:35.537934Z",
     "start_time": "2018-11-02T15:20:35.448397Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def remove_punctuation(text):\n",
    "    regex = re.compile('[%s]' % re.escape(string.punctuation))\n",
    "    new_text = [regex.sub('', word) for word in text]\n",
    "    return new_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-02T15:22:55.730531Z",
     "start_time": "2018-11-02T15:22:55.725747Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def stemming(text):\n",
    "    text = [word for word in text if word != \"\"]\n",
    "    stemmer = RSLPStemmer()\n",
    "    new_text = [stemmer.stem(word) for word in text]\n",
    "    return new_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-02T15:22:56.560143Z",
     "start_time": "2018-11-02T15:22:56.556098Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def lowercase(text):\n",
    "    new_text = [word.lower() for word in text]\n",
    "    return new_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-02T15:22:56.938519Z",
     "start_time": "2018-11-02T15:22:56.933540Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def lemmatization(text):\n",
    "    nlp = spacy.load('pt')\n",
    "    new_text = []\n",
    "    for word in text:\n",
    "        token = nlp(word)[0]\n",
    "        if token.pos_ == 'VERB':\n",
    "            new_text += [token.lemma_ ]\n",
    "        else:\n",
    "            new_text +=[word]\n",
    "    return new_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-02T15:22:57.297360Z",
     "start_time": "2018-11-02T15:22:57.293145Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def tokenize_text(sentences):\n",
    "    words = [tokenize.word_tokenize(sent, language='portuguese') for sent in sentences]\n",
    "    words = sum(words, [])\n",
    "    return words\n",
    "    \n",
    "    \n",
    "def preprocess(text):\n",
    "    assert type(text) == list, \"input must be a list\"\n",
    "    text_lower = lowercase(text)\n",
    "    text_punc = remove_punctuation(text_lower)\n",
    "    text_stop = remove_stopwords(text_punc)\n",
    "    text_stem = stemming(text_stop)\n",
    "    text_lemma  = lemmatization(text_stem)\n",
    "    return text_lemma"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-02T15:22:58.195075Z",
     "start_time": "2018-11-02T15:22:58.189736Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Eu', ',', 'qUeRo', ',', 'uM', ':', 'chocoLATe', '?', 'coM/Caramelo', '.', 'e', 'um', 'gato', 'gordo', ',', 'por', 'favor']\n"
     ]
    }
   ],
   "source": [
    "text = [\"\".join([\"Eu,\", \"qUeRo,\", \"uM:\", \"chocoLATe?\", \"coM/\", \"Caramelo.\"]),\n",
    "        \"e um gato gordo, por favor\"]\n",
    "text = tokenize_text(text)\n",
    "print(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-02T15:22:58.747537Z",
     "start_time": "2018-11-02T15:22:58.741935Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Eu', ',', 'qUeRo', ',', 'uM', ':', 'chocoLATe', '?', 'coM/Caramelo', '.', 'gato', 'gordo', ',', 'favor']\n"
     ]
    }
   ],
   "source": [
    "print(remove_stopwords(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-02T15:22:59.394157Z",
     "start_time": "2018-11-02T15:22:59.389467Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Eu', '', 'qUeRo', '', 'uM', '', 'chocoLATe', '', 'coMCaramelo', '', 'e', 'um', 'gato', 'gordo', '', 'por', 'favor']\n"
     ]
    }
   ],
   "source": [
    "no_punc = remove_punctuation(text)\n",
    "print(no_punc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-02T15:23:00.001997Z",
     "start_time": "2018-11-02T15:22:59.991764Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['eu',\n",
       " 'quer',\n",
       " 'um',\n",
       " 'chocolat',\n",
       " 'comcaramel',\n",
       " 'e',\n",
       " 'um',\n",
       " 'gat',\n",
       " 'gord',\n",
       " 'por',\n",
       " 'favor']"
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stemming(no_punc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-02T15:23:46.278775Z",
     "start_time": "2018-11-02T15:23:46.269002Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['eu',\n",
       " ',',\n",
       " 'quero',\n",
       " ',',\n",
       " 'um',\n",
       " ':',\n",
       " 'chocolate',\n",
       " '?',\n",
       " 'com/caramelo',\n",
       " '.',\n",
       " 'e',\n",
       " 'um',\n",
       " 'gato',\n",
       " 'gordo',\n",
       " ',',\n",
       " 'por',\n",
       " 'favor']"
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lowercase(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-02T15:23:51.375965Z",
     "start_time": "2018-11-02T15:23:50.533153Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Eu',\n",
       " ',',\n",
       " 'qUeRo',\n",
       " ',',\n",
       " 'uM',\n",
       " ':',\n",
       " 'chocoLATe',\n",
       " '?',\n",
       " 'coM/Caramelo',\n",
       " '.',\n",
       " 'e',\n",
       " 'um',\n",
       " 'gato',\n",
       " 'gordo',\n",
       " ',',\n",
       " 'por',\n",
       " 'favor']"
      ]
     },
     "execution_count": 108,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lemmatization(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-02T15:24:01.109112Z",
     "start_time": "2018-11-02T15:24:00.484275Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['querer', 'chocolat', 'comcaramel', 'gat', 'gord', 'favor']\n"
     ]
    }
   ],
   "source": [
    "print(preprocess(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-10-23T20:08:12.872183Z",
     "start_time": "2018-10-23T20:08:12.850426Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Parser:\n",
    "    def extract(url):\n",
    "        pass\n",
    "    \n",
    "    def parse(tags_dict):\n",
    "        pass\n",
    "    \n",
    "    def get_wdlist(url):\n",
    "        pass\n",
    "        \n",
    "    def preprocess(text):\n",
    "        assert type(text) == list, \"input must be a list\"\n",
    "        text_lower = lowercase(text)\n",
    "        text_punc = remove_ponctuation(text_lower)\n",
    "        text_stop = remove_stopwords(text_punc)\n",
    "        text_stem = stemming(text_stop)\n",
    "        text_lemma  = lemmatization(text_stem)\n",
    "        return text_lemma              "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-10-23T17:32:03.279005Z",
     "start_time": "2018-10-23T17:32:03.255327Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import unittest as ut"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-10-23T20:11:01.150383Z",
     "start_time": "2018-10-23T20:11:00.130974Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/larissa/anaconda3/lib/python3.6/site-packages/msgpack_numpy.py:184: PendingDeprecationWarning: encoding is deprecated, Use raw=False instead.\n",
      "  return _unpackb(packed, **kwargs)\n",
      "../home/larissa/anaconda3/lib/python3.6/site-packages/nltk/corpus/reader/wordlist.py:28: ResourceWarning: unclosed file <_io.BufferedReader name='/home/larissa/nltk_data/corpora/stopwords/portuguese'>\n",
      "  return concat([self.open(f).read() for f in fileids])\n",
      ".....\n",
      "----------------------------------------------------------------------\n",
      "Ran 7 tests in 0.978s\n",
      "\n",
      "OK\n"
     ]
    }
   ],
   "source": [
    "class TestPreProcessTextMethods(ut.TestCase):\n",
    "    \n",
    "    def test_remove_stopwords(self):\n",
    "        self.assertEqual(remove_stopwords(['este', 'é',  'um',  'teste']), ['é', 'teste'])\n",
    "        self.assertEqual(remove_stopwords(['Este', 'é', 'UM', 'teste.']), ['Este', 'é', 'UM', 'teste.'])\n",
    "    \n",
    "    def test_remove_punctuation(self):\n",
    "        self.assertEqual(remove_punctuation(['Este', 'é', 'UM', 'teste.']), ['Este', 'é', 'UM', 'teste'])\n",
    "    \n",
    "    def test_stemming(self):\n",
    "        self.assertEqual(stemming(['sabia', 'pedraria', 'casarão', 'ferreiro']), ['sab', 'pedr', 'cas', 'ferr'])\n",
    "    \n",
    "    def test_lowercase(self):\n",
    "        self.assertEqual(lowercase(['TEmoS', 'QUE', 'estAr', 'minÚscULas']), ['temos', 'que', 'estar', 'minúsculas'])\n",
    "        \n",
    "    def test_lemmatization(self):\n",
    "        self.assertEqual(lemmatization(['é', 'quero', 'sei', 'vemos', 'lê']), ['ser', 'querer', 'saber', 'ver', 'ler'])\n",
    "    \n",
    "    def test_preprocess(self):\n",
    "        self.assertEqual(preprocess(['Este', 'é', 'uM.', 'tesTe?', 'das', 'FUNÇÔES', 'ACima,', 'em', 'conjunto%']), \n",
    "                         ['ser', 'test', 'funçô', 'acim', 'conjunt'])\n",
    "        \n",
    "    def test_preprocess_notlist(self):\n",
    "        with self.assertRaises(Exception) as context:\n",
    "            preprocess(\"este é um teste\") \n",
    "        self.assertTrue('input must be a list' in str(context.exception))\n",
    "       \n",
    "    \n",
    "# if __name__ == '__main__':\n",
    "#     ut.main() only for script\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    ut.main(argv=['first-arg-is-ignored'], exit=False)\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-10-23T17:38:38.900063Z",
     "start_time": "2018-10-23T17:38:38.881842Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['é', 'teste']"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "remove_stopwords(['este', 'é', 'um', 'teste'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-10-23T17:38:21.243901Z",
     "start_time": "2018-10-23T17:38:21.233072Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False\n"
     ]
    }
   ],
   "source": [
    "print('Este' in stopwords.words('portuguese'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
